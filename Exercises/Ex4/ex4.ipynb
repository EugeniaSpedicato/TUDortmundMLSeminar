{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Fully Connected Networks and the MNIST dataset\n",
    "This exercise is based on https://github.com/leriomaggio/deep-learning-keras-tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST database\n",
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) database ([link](http://yann.lecun.com/exdb/mnist)) has a database of handwritten digits. The dataset  consists of 28x28 grayscale images of the 10 digits.\n",
    "\n",
    "![](mnist.png)\n",
    "\n",
    "Since this dataset is **provided** with Keras, we just ask the `keras.dataset` model for training and test data.\n",
    "\n",
    "`from keras.datasets import mnist`<br>\n",
    "`(X_train, y_train), (X_test, y_test) = mnist.load_data()`\n",
    "\n",
    "The training set has $60,000$ samples. \n",
    "The test set has $10,000$ samples.\n",
    "The digits are size-normalized and centered in a fixed-size image. \n",
    "The data page has description on how the data was collected. It also has reports the benchmark of various algorithms on the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data preparation \n",
    "* Download the data\n",
    "* Inspect the data and plot a few of the images using `matplotlib.pyplot.imshow` \n",
    "* Reshape the input data to be in vectorial form (original data are images)\n",
    "* Convert the input data to do dtype `float32` using `astype` in order to scale it afterwards\n",
    "* Normalize the design matrix to values between 0 and 1.\n",
    "* How many classes do you have? How much data of each class?\n",
    "* Convert the class vector to binary class matrices (**one-hot-vector**). Use the `to_categorical` function from `keras.utilis` to convert integer labels to **one-hot-vectors**.\n",
    "* Split the training set into training and validation data (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build and train a neural network\n",
    "* Design a dense neural network structure. \n",
    "* Choose `softmax` as activation for the output node (normalized multi-class probability)\n",
    "* Use `categorical_crossentropy` as loss function (multi-class version of crossentropy)\n",
    "* Use `adam` as optimizer and a batch size of 512 (speed things up)\n",
    "* Train the NN over 50 epochs and plot the evolution of the training and validation loss as well as of one meaningful metric. What do you observe?\n",
    "* Evaluate the performance on the test set using `sklearn.metrics`\n",
    "* Plot the probability of being a *Zero* for true zeros and for all other numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to show the performance of a multi-class output is the confusion matrix: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note, this code is taken straight from the SKLEARN website, an nice way of viewing confusion matrix.\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_cls) \n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10))\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10), normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot wrong associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Errors are difference between predicted labels and true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = (Y_cls - Y_true != 0)\n",
    "\n",
    "Y_cls_errors = Y_cls[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_test_errors = X_test[errors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)), cmap=cm.Greys, interpolation='nearest')\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank errors by difference in probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_test_errors, Y_cls_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dropout Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned last time, the trainings and validation loss of the fit history is not comparable when using dropout. We can define our own callback function which calculates the loss and metric after each epoch for any dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class HistoryEpoch(Callback):\n",
    "    def __init__(self, data):\n",
    "        self.data = data        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.data\n",
    "        l, a = self.model.evaluate(x, y, verbose=0)\n",
    "        self.loss.append(l)\n",
    "        self.acc.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Using regularizer\n",
    "\n",
    "* Modify your previous example network by adding a Dropout layer after each hidden layer\n",
    "* Add l2 regularization to the hidden layers\n",
    "* Use the new defined `HistoryEpoch` for training, validation and test data set in order to save a comparable loss function and metric. This is done by e.g.: `train_hist=HistoryEpoch((X_train, Y_train))`. In the `fit` function you can call the callback then by specifying `callbacks=[train_hist]`.\n",
    "* Plot the loss and metric evolution and compare the calculated loss with the default loss from the history\n",
    "* Evaluate the performance of the NN as for the unregularized NN and compare the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping as a regularizer\n",
    "\n",
    "* If you continue training, at some point the validation loss will start to increase: that is when the model starts to **overfit**. We can use EarlyStopping as a regularizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "#Also possible choice:\n",
    "#early_stop = EarlyStopping(monitor='val_acc', patience=5, verbose=1)\n",
    "\n",
    "dropout=0.5\n",
    "\n",
    "model_ES = Sequential()\n",
    "model_ES.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_lambda), input_dim=784))\n",
    "model_ES.add(Dropout(dropout))\n",
    "model_ES.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "model_ES.add(Dropout(dropout))\n",
    "model_ES.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_ES.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_dropout.summary()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_ES = model_ES.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs=100, batch_size=256, verbose=1, \n",
    "             callbacks=[early_stop]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Inspecting Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already used `summary`\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model.layers` is iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Input Tensors: ', model.input)\n",
    "print('Layers - Network Configuration:')\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    print('Layer Configuration:')\n",
    "    print(layer.get_config(), )\n",
    "print('Model Output Tensors: ', model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hidden layer representation of the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **simple** way to do it is to use the weights of your model to build a new model that's truncated at the layer you want to read. \n",
    "\n",
    "Then you can run the `._predict(X_batch)` method to get the activations for a batch of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_truncated = Sequential()\n",
    "model_truncated.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model_truncated.add(Dropout(dropout))\n",
    "model_truncated.add(Dense(256, activation='relu'))\n",
    "\n",
    "for i, layer in enumerate(model_truncated.layers):\n",
    "    layer.set_weights(model_dropout.layers[i].get_weights())\n",
    "\n",
    "model_truncated.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "np.all(model_truncated.layers[0].get_weights()[0] == model.layers[0].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_features = model_truncated.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: Alternative Method to get activations \n",
    "\n",
    "(Using `keras.backend` `function` on Tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "def get_activations(model, layer, X_batch):\n",
    "    activations_f = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\n",
    "    activations = activations_f((X_batch, False))\n",
    "    return activations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Embedding of Hidden Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction to dim=20 by using principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "pca_result = pca.fit_transform(hidden_features)\n",
    "print('Variance PCA: {}'.format(np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction to dim=2 by using t-distributed stochastic neighbor embedding (TSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(pca_result[:1000]) ## Reduced for computational issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors_map = np.argmax(Y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(colors_map==6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([x for x in 'b-g-r-c-m-y-k-purple-coral-lime'.split('-')])\n",
    "colors_map = np.argmax(Y_train, axis=1)\n",
    "colors_map = colors_map[:1000]\n",
    "plt.figure(figsize=(10,10))\n",
    "for cl in range(nb_classes):\n",
    "    indices = np.where(colors_map==cl)\n",
    "    plt.scatter(X_tsne[indices,0], X_tsne[indices, 1], c=colors[cl], label=cl)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
