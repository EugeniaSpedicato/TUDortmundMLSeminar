{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is based on https://github.com/leriomaggio/deep-learning-keras-tensorflow, https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ and [https://github.com/fchollet/keras/blob/master/examples/mnist_sklearn_wrapper.py]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build simple CNN models to classify the MNIST dataset and uses sklearn's GridSearchCV to find the best hyperparameter model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load and preprocess the data as we did in exercise 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1338)  # for reproducibilty!!\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of classes\n",
    "nb_classes = 10\n",
    "\n",
    "#Data format\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    shape_ord = (1, img_rows, img_cols)\n",
    "else:  # channel_last\n",
    "    shape_ord = (img_rows, img_cols, 1)\n",
    "    \n",
    "#Load the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Scale the data and concert to data format\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0],img_rows*img_cols))\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0],img_rows*img_cols))\n",
    "X_train = X_train.reshape(X_train.shape[0],img_rows,img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0],img_rows,img_cols)\n",
    "X_train = X_train.reshape((X_train.shape[0],) + shape_ord)\n",
    "X_test = X_test.reshape((X_test.shape[0],) + shape_ord)\n",
    "\n",
    "#convert target vector\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "#Take just 20k example for training for speed reasons\n",
    "X_train = X_train[:20000]\n",
    "Y_train = Y_train[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use Keras Models in scikit-learn\n",
    "Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class.\n",
    "\n",
    "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the build_fn argument when constructing the KerasClassifier class.\n",
    "\n",
    "You can learn more about the scikit-learn wrapper in Keras API documentation:\n",
    "\n",
    "https://keras.io/scikit-learn-api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "We will define a function which builds a similar model we used in exercise 5, but depends on all hyperparameter we would like to tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_model(dense_layer_sizes, dense_activation, filters, \n",
    "               kernel_size, pool_size, padding_type, stride_size, dropout_rate, optimizer):\n",
    "    '''Creates model comprised of 2 convolutional layers followed by dense layers\n",
    "\n",
    "    dense_layer_sizes: List of layer sizes. This list has one number for each layer\n",
    "    dense_activation: activation funciton in dense layer\n",
    "    filters: Number of convolutional filters in each convolutional layer\n",
    "    kernel_size: Convolutional kernel size\n",
    "    pool_size: Size of pooling area for max pooling\n",
    "    padding_type: type of padding: same or valid\n",
    "    stride_size: symmetric stride size\n",
    "    dropout_rate: dropout rate\n",
    "    optimizer: optimizer used for mimizing\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters, (kernel_size, kernel_size), padding=padding_type, \n",
    "                     strides=(stride_size, stride_size), activation='relu', input_shape=shape_ord))\n",
    "    model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    for layer_size in dense_layer_sizes:\n",
    "        model.add(Dense(layer_size, activation=dense_activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_cnn = KerasClassifier(make_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Not even all hyperparameter are included here, we are not varying things like the neural network weight initialization\n",
    "\n",
    "`model.add(Dense(layer_size, activation=dense_activation, kernel_initializer=init_mode))` <br>\n",
    "with <br>\n",
    "`init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']` <br>\n",
    "\n",
    "or we have not included kernel regularizer like l1/l2 for which we would need to change the strength. The parameter of the optimizer are also not yet included, but we will do that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use Grid Search in scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is a model hyperparameter optimization technique. More information on hyperparameter optimization can be found here:\n",
    "http://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "In scikit-learn this technique is provided in the `GridSearchCV` class.\n",
    "\n",
    "When constructing this class you must provide a dictionary of hyperparameters to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to try.\n",
    "\n",
    "By default, accuracy is the score that is optimized, but other scores can be specified in the score argument of the GridSearchCV constructor.\n",
    "\n",
    "By default, the grid search will only use one thread. By setting the n_jobs argument in the GridSearchCV constructor to -1, the process will use all cores on your machine. Depending on your Keras backend, this may interfere with the main neural network training process.\n",
    "\n",
    "The GridSearchCV process will then construct and evaluate one model for each combination of parameters. Cross validation is used to evaluate each individual model and by default a 3-fold cross validation is used, although this can be overridden by specifying the cv argument to the GridSearchCV constructor.\n",
    "Below is an example of defining a simple grid search:\n",
    "\n",
    "`param_grid = dict(epochs=[10,20,30])`<br>\n",
    "`grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)`<br>\n",
    "`grid_result = grid.fit(X, Y)`\n",
    "\n",
    "Once completed, you can access the outcome of the grid search in the result object returned from grid.fit(). The best_score_ member provides access to the best score observed during the optimization procedure and the best_params_ describes the combination of parameters that achieved the best results.\n",
    "\n",
    "You can learn more about it here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch HyperParameters\n",
    "First we would like to optimize the convolutional part of the NN, we fix everything else and vary the filter, kernel size, pool size. We could change the padding type and the striding, but we know that padding won't have a large impact, since there is no information in the corners and we neglect striding for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_size_candidates = [[32]]\n",
    "optimizer = ['Adam']\n",
    "activation = ['relu']\n",
    "\n",
    "param_grid={'dense_layer_sizes': dense_size_candidates,\n",
    "            'dense_activation' : activation,\n",
    "             'filters': [16, 32],\n",
    "             'kernel_size': [3, 5],\n",
    "             'pool_size': [2, 4],\n",
    "             'padding_type' : ['valid'],\n",
    "             'stride_size'  : [1],\n",
    "             'dropout_rate' : [0.5],\n",
    "             'optimizer' : optimizer,\n",
    "             # epochs and batch_size are avail for tuning even when not\n",
    "             # an argument to model building function\n",
    "             'epochs': [1],\n",
    "             'batch_size': [256]\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save the best model for each grid search step (we could also save all of them, of course). For that we will use the ModelCheckpoint call back function, which is also useful to save a NN model after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(filepath='mnist_example.h5',save_best_only=True)\n",
    "\n",
    "filepath = \"best_cnn.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cross-validation with k=2 (speed) and `average precision` as a score value to find the best parameter set. The best metric for optimization depends on your problem, you can find a built-in list here:<br>\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter<br>\n",
    "But you can also define your own scoring function:<br>\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#defining-your-scoring-strategy-from-metric-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(my_cnn, param_grid, cv=2, scoring='average_precision', n_jobs=1)\n",
    "grid_result = grid.fit(X_train, Y_train, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Perform a sequentiell grid search to optimze the following hyperparameter. Save the best model for each of the sequentiell steps into a hdf5 file.\n",
    "* Find the best optimizer: `'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'`\n",
    "* Find the best dense structure: Change the width and the depth and try at least 4 different structures\n",
    "* Find the best activation function for the dense network `'softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'`\n",
    "* Find the best dropout rate: Change dropout between 0 and 0.5 in 0.1 steps\n",
    "* Find the best optimizer parameter: In order to do that, you need to change the make_model function to adapt for that. Vary the parameter in a meaningful range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Hyperparameter Optimization\n",
    "This section lists some handy tips to consider when tuning hyperparameters of your neural network.\n",
    "\n",
    "* **k-fold Cross Validation.** You can see that the results from the examples in this post show some variance. For speed reasons, we used a cross-validation of 2, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.\n",
    "* **Review the Whole Grid.** Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.\n",
    "* **Parallelize.** Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider using cluster instances if available.\n",
    "* **Use a Sample of Your Dataset.** Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.\n",
    "* **Start with Coarse Grids.** Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.\n",
    "* **Do not Transfer Results.** Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.\n",
    "* **Reproducibility is a Problem.** Although we set the seed for the random number generator in NumPy, the results are not 100% reproducible. There is more to reproducibility when grid searching wrapped Keras models than is presented in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Load the best model and evaluate it using the function below\n",
    "You can load the model using<br>\n",
    "`from keras.models import load_model`<br>\n",
    "`model= load_model('filename')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################################### \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['acc'])\n",
    "    plt.plot(network_history.history['val_acc'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "###################################################################################################    \n",
    "\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "################################################################################################### \n",
    "import matplotlib.cm as cm\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)), cmap=cm.Greys, interpolation='nearest')\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "            \n",
    "################################################################################################### \n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "def evaluate(X_test, Y_test, model):\n",
    "    \n",
    "    ##Evaluate loss and metrics\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test Loss:', loss)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    # Predict the values from the test dataset\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # Convert predictions classes to one hot vectors \n",
    "    Y_cls = np.argmax(Y_pred, axis = 1) \n",
    "    # Convert validation observations to one hot vectors\n",
    "    Y_true = np.argmax(Y_test, axis = 1) \n",
    "    print 'Classification Report:\\n', classification_report(Y_true,Y_cls)\n",
    "    \n",
    "    ## Plot 0 probability including overtraining test\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    label=0\n",
    "    #Test prediction\n",
    "    Y_pred_prob = Y_pred[:,label]\n",
    "    plt.hist(Y_pred_prob[Y_true == label], alpha=0.5, color='red', range=[0, 1], bins=10, log = True)\n",
    "    plt.hist(Y_pred_prob[Y_true != label], alpha=0.5, color='blue', range=[0, 1], bins=10, log = True)\n",
    "    #Train prediction\n",
    "    Y_train_pred = model.predict(X_train)\n",
    "    Y_train_pred_prob = Y_train_pred[:,label]\n",
    "    Y_train_true = np.argmax(Y_train, axis = 1) \n",
    "    plt.hist(Y_train_pred_prob[Y_train_true == label], alpha=0.5, color='red', range=[0, 1], bins=10, log = True, histtype='step', linewidth=2)\n",
    "    plt.hist(Y_train_pred_prob[Y_train_true != label], alpha=0.5, color='blue', range=[0, 1], bins=10, log = True, histtype='step', linewidth=2)\n",
    "    \n",
    "    plt.legend(['train == 0', 'train != 0', 'test == 0', 'test != 0'], loc='upper right')\n",
    "    plt.xlabel('Probability of being 0')\n",
    "    plt.ylabel('Number of entries')\n",
    "    plt.show()\n",
    "    \n",
    "    # compute the confusion matrix\n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_cls) \n",
    "    # plot the confusion matrix\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plot_confusion_matrix(confusion_mtx, classes = range(10))\n",
    "\n",
    "    #Plot largest errors\n",
    "    errors = (Y_cls - Y_true != 0)\n",
    "    Y_cls_errors = Y_cls[errors]\n",
    "    Y_pred_errors = Y_pred[errors]\n",
    "    Y_true_errors = Y_true[errors]\n",
    "    X_test_errors = X_test[errors]\n",
    "    # Probabilities of the wrong predicted numbers\n",
    "    Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "    # Predicted probabilities of the true values in the error set\n",
    "    true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "    # Difference between the probability of the predicted label and the true label\n",
    "    delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "    # Sorted list of the delta prob errors\n",
    "    sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "    # Top 6 errors \n",
    "    most_important_errors = sorted_dela_errors[-6:]\n",
    "    # Show the top 6 errors\n",
    "    display_errors(most_important_errors, X_test_errors, Y_cls_errors, Y_true_errors)\n",
    "    \n",
    "    ##Plot predictions\n",
    "    slice = 15\n",
    "    predicted = model.predict(X_test[:slice]).argmax(-1)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    for i in range(slice):\n",
    "        plt.subplot(1, slice, i+1)\n",
    "        plt.imshow(X_test[i].reshape(28,28), interpolation='nearest')\n",
    "        plt.text(0, 0, predicted[i], color='black', \n",
    "                 bbox=dict(facecolor='white', alpha=1))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There's more:\n",
    "\n",
    "The `GridSearchCV` model in scikit-learn performs a complete search, considering **all** the possible combinations of Hyper-parameters we want to optimise.\n",
    "\n",
    "If we want to apply for an optmised and bounded search in the hyper-parameter space, I strongly suggest to take a look at:\n",
    "\n",
    "* `Keras + hyperopt == hyperas`: [http://maxpumperla.github.io/hyperas/](http://maxpumperla.github.io/hyperas/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
